{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script for uploading CSV to Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import psycopg2\n",
    "from psycopg2 import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"../data/weather_station_list/data/weather_sta_list_ENG.csv\"\n",
    "conn_string = \"postgresql://lizavabistsevits:@localhost:5432/taiwan\" # database connection string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_csv_rows(csv_file):\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        \n",
    "        # Create dictionary for each row\n",
    "        for row in csv_reader:\n",
    "            print(row)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_csv_rows(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open database connection\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results: []\n"
     ]
    }
   ],
   "source": [
    "# 1. Execute a query\n",
    "cursor.execute(\"SELECT * FROM stations LIMIT 5;\")\n",
    "\n",
    "# 2. Fetch results\n",
    "# Fetch all rows\n",
    "all_results = cursor.fetchall()\n",
    "print(\"All results:\", all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_csv_to_existing_db_table(csv_file, table_name, conn_string, column_names):\n",
    "    # Connect to database\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create a set to record processed entries\n",
    "    processed_codes = set()\n",
    "    duplicates = 0\n",
    "    \n",
    "    # Find code column if it exists\n",
    "    code_column = next((col for col in column_names.keys() if col.lower() == \"code\"), None)\n",
    "    \n",
    "    with open(csv_file, 'r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        \n",
    "        # Insert data row by row\n",
    "        for row in csv_reader:\n",
    "            # Check for duplicate codes\n",
    "            if code_column and code_column in row:\n",
    "                code_value = row[code_column]\n",
    "                # Skip this row if the code has been already processed\n",
    "                if code_value in processed_codes:\n",
    "                    duplicates += 1\n",
    "                    continue\n",
    "                # Add to the tracking set\n",
    "                processed_codes.add(code_value)\n",
    "            \n",
    "            # Prepare column names and values for the specific columns\n",
    "            columns = []\n",
    "            values = []\n",
    "            \n",
    "            for csv_col, db_col in column_names.items():\n",
    "                if csv_col in row:\n",
    "                    columns.append(db_col)\n",
    "                    # Convert empty strings to NULL\n",
    "                    if row[csv_col] == \"\" or row[csv_col] is None:\n",
    "                        values.append(None)\n",
    "                    else:\n",
    "                        values.append(row[csv_col])\n",
    "            \n",
    "            # Skip if no valid columns to insert\n",
    "            if not columns:\n",
    "                continue\n",
    "            \n",
    "            # Create the INSERT query with only the specified columns\n",
    "            columns_str = sql.SQL(', ').join(sql.Identifier(col) for col in columns)\n",
    "            placeholders = sql.SQL(', ').join([sql.SQL('%s')] * len(values))\n",
    "            \n",
    "            # Use sql.SQL to safely handle table and column names\n",
    "            insert_query = sql.SQL('INSERT INTO {} ({}) VALUES ({});').format(\n",
    "                sql.Identifier(table_name),\n",
    "                columns_str,\n",
    "                placeholders\n",
    "            )\n",
    "            \n",
    "            # Execute the query\n",
    "            cursor.execute(insert_query, values)\n",
    "    \n",
    "    # Commit changes and close connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Data from {csv_file} successfully imported to {table_name} table.\")\n",
    "    print(f\"Processed {len(processed_codes)} unique entries and skipped {duplicates} duplicates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ../data/weather_station_list/data/weather_sta_list_ENG.csv successfully imported to stations table.\n",
      "Processed 1155 unique entries and skipped 80 duplicates.\n"
     ]
    }
   ],
   "source": [
    "columns = {\n",
    "    \"Code\": \"code\",\n",
    "    \"Name\": \"name\",\n",
    "    \"Original_Name\" : \"orig_name\",\n",
    "    \"Type\": \"type\",\n",
    "    \"Latitude\": \"latitude\",\n",
    "    \"Longitude\": \"longitude\",\n",
    "\t\"Altitude\" : \"altitude\",\n",
    "\t\"Data_Start_Date\" : \"data_start_date\",\n",
    "\t\"Data_End_Date\" : \"data_end_date\"\n",
    "}\n",
    "\n",
    "upload_csv_to_existing_db_table(csv_file_path, \"stations\", conn_string, columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dengue-data-exploration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
